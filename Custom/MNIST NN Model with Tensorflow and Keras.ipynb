{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28 x 28 hand-written images of digits 0-9. \n",
    "mnist = tf.keras.datasets.mnist          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() # split the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]   # Each pixel of the image has intensity between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize (scale) the training and testing sets so that the pixel intensity is normalized between 0 and 1.\n",
    "# Normalizing the data generally speeds up learning and leads to faster convergence.\n",
    "\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]   # Each pixel value is now normalized between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Model\n",
    " \n",
    "model = tf.keras.models.Sequential() # Sequential groups a linear stack of layers into a model.\n",
    "model.add(tf.keras.layers.Flatten()) # This is the input layer flattened as the original images are 28 x 28.\n",
    "\n",
    "# We are using 2 hidden layers with 128 units (neurons) with ReLU as the activation function. Activation function\n",
    "# of a node is used to define the output of that node given an input or set of inputs.\n",
    "# ReLU (rectified linear) is widely used and is default choice as it yields better results.\n",
    "model.add(tf.keras.layers.Dense(units=128,activation=tf.nn.relu)) \n",
    "model.add(tf.keras.layers.Dense(units=128,activation=tf.nn.relu))\n",
    "\n",
    "# This is the output layer. We use units=10 because this is a classfication problem and we have 10 classes\n",
    "# Softmax converts a vector of numbers into a vector of probabilities (probability distribution). It is commonly\n",
    "# used as an activation function in a neural network for classification problems.\n",
    "model.add(tf.keras.layers.Dense(units=10,activation=tf.nn.softmax))\n",
    "\n",
    "# Now, lets configure the model for training. \n",
    "# Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order \n",
    "# and second-order moments.\n",
    "# The neural network will optimize to minimize the loss function. Sparse categorical crossentropy produces \n",
    "# a category index of the most likely matching category. \n",
    "# The metric to be evaluated during training is accuracy.\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Finally, lets train the model. \n",
    "# Running one epoch is like training a neural network with the entire training data for one cycle.\n",
    "# As we are running 4 epochs, that is like doing 4 cycles on all the training data.  \n",
    "model.fit(x_train,y_train,epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on unseen test set.\n",
    "predictions = model.predict(x_test)\n",
    "y_pred = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted - Calculate metrics for each label, and find their average weighted by support (the number of true \n",
    "# instances for each label). \n",
    "\n",
    "precision_score(y_test, y_pred , average=\"weighted\") # Compute precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred , average=\"weighted\") # Compute recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "label_names=['0','1','2','3','4','5','6','7','8','9']\n",
    "cm =confusion_matrix(y_test, y_pred)\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cmn = cmn * 100\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=label_names, yticklabels=label_names)\n",
    "plt.ylabel('Actual labels')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a sample test image and its predicted value\n",
    "plt.imshow(x_test[0], cmap=plt.cm.binary)\n",
    "plt.xlabel('Actual Digit')\n",
    "print(\"Actual Digit :\")\n",
    "plt.show()\n",
    "print(\"Predicted Digit : \", y_pred[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
